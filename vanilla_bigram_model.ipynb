{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Bigram model \n",
    "\n",
    "#### by Jesse Elliot & Alina Gonzalez\n",
    "\n",
    "We want to explore how the different neural networks and different emotions compare to a simple vanilla bigram model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from utils import LanguageModel\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# constants\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "EMOTION_KEY = {0: 'anger', 1: 'fear', 2: 'joy', 3:'love', 4:'sadness', 5: 'surprise'} # manually done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility funcs \n",
    "**some functions are provided from HW3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "  \"\"\"Creates n-grams for the given token sequence.\n",
    "  Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "  Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "  \"\"\"\n",
    "  lst = []\n",
    "  for i in range(len(tokens)): \n",
    "    ngrams = []\n",
    "    for j in range(i, i+n): \n",
    "      try: \n",
    "        ngrams.append(tokens[j])\n",
    "      except: \n",
    "        break\n",
    "    if len(ngrams) == n: \n",
    "      lst.append(tuple(ngrams))\n",
    "  return lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line: str, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize a single string. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    line (str): text to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - a single line tokenized\n",
    "  \"\"\"\n",
    "  inner_pieces = None\n",
    "  if by_char:\n",
    "    inner_pieces = list(line)\n",
    "  else:\n",
    "    # otherwise split on white space\n",
    "    inner_pieces = line.split()\n",
    "\n",
    "  if ngram == 1:\n",
    "    tokens = [sentence_begin] + inner_pieces + [sentence_end]\n",
    "  else:\n",
    "    tokens = ([sentence_begin] * (ngram - 1)) + inner_pieces + ([sentence_end] * (ngram - 1))\n",
    "  # always count the unigrams\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: list, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize each line in a list of strings. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    data (list): list of strings to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - all lines tokenized as one large list\n",
    "  \"\"\"\n",
    "  total = []\n",
    "  # also glue on sentence begin and end items\n",
    "  for line in data:\n",
    "    line = line.strip()\n",
    "    # skip empty lines\n",
    "    if len(line) == 0:\n",
    "      continue\n",
    "    tokens = tokenize_line(line, ngram, by_char, sentence_begin, sentence_end)\n",
    "    total += tokens\n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('data/training.txt', 'r') as file: \n",
    "    for line in file: \n",
    "        text.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "df = pd.read_csv('data/training.csv')\n",
    "\n",
    "emotion_dfs = {}\n",
    "for label in EMOTION_KEY: \n",
    "    curr = df.loc[df['label'] == label]\n",
    "    emotion_dfs[label] = curr\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3670e09b5b1a4d639f3bbaabb6fb5b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram = 2\n",
    "gen_sents = {}\n",
    "for label, df_ in tqdm(emotion_dfs.items()): \n",
    "    # tokenize by word\n",
    "    toks = tokenize(list(df_['text']), ngram, by_char=False)\n",
    "    lm = LanguageModel(ngram)\n",
    "    lm.train(toks)\n",
    "    gen_sents[label] = lm.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vanilla_emotion_results.txt', 'w') as file: \n",
    "    json.dump(gen_sents, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
