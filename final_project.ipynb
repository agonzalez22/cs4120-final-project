{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Analysis in Tweets \n",
    "by Jesse Elliott and Alina Gonzalez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our data \n",
    "For this project we utilized [this](https://github.com/dair-ai/emotion_dataset) dataset with sampled tweets + preprocessed emotions as labels. \n",
    "\n",
    "__The labels were as provided:__\n",
    "Anger : 0\n",
    "Fear : 1\n",
    "Joy : 2\n",
    "Love : 3\n",
    "Sadness : 4\n",
    "Surprise : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from collections import Counter \n",
    "import math \n",
    "import random \n",
    "import time \n",
    "from utils import LanguageModel \n",
    "from tqdm.notebook import tqdm \n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "EMOTION_KEY = {0: 'anger', 1: 'fear', 2: 'joy', 3:'love', 4:'sadness', 5: 'surprise'} # manually done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training.csv')\n",
    "df['label_text'] = [EMOTION_KEY[_] for _ in df['label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data frame to .txt file \n",
    "path = 'data/training.txt'\n",
    "\n",
    "with open(path, 'w') as file: \n",
    "    for line in list(df['text']): \n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df[\"label_text\"]) # everyone is afraid i guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balancing the data\n",
    "\n",
    "We came to the conclusion that we needed to balance the data in order to prevent our model from underperforming with certain emotions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of data points for the 'fear' class\n",
    "fear_count = df[df['label'] == 1].shape[0]\n",
    "\n",
    "# Get the data points for each emotion class with the same count as 'fear'\n",
    "balanced_data = pd.concat([df[df['label'] == label].sample(fear_count, replace=True, random_state=42) \n",
    "                           for label in range(6)])\n",
    "\n",
    "# Save the balanced data to a new CSV file\n",
    "balanced_data.to_csv('data/balanced_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data frame to .txt file \n",
    "path = 'data/balanced_training.txt'\n",
    "\n",
    "with open(path, 'w') as file: \n",
    "    for line in list(balanced_data['text']): \n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(balanced_data[\"label_text\"]) # thats better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the data! \n",
    "\n",
    "We want to understand what words define an emotion! Lets make a word cloud to get a visual representation of just that! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(lst:list, stopwords):\n",
    "    \"\"\" parses a list of strings representing tweets \"\"\" \n",
    "    valid_total = []\n",
    "    for tweet in lst: \n",
    "        tweet = tweet.split()\n",
    "        valid = [word for word in tweet if word not in stopwords]\n",
    "        valid_total.append(\" \".join(valid))\n",
    "    return \" \".join(valid_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(list(STOPWORDS) + ['feel', 'feeling', 'ive', 'im'])\n",
    "\n",
    "# generate a word cloud for every... label\n",
    "for label in EMOTION_KEY.keys(): \n",
    "    sub_df = balanced_data.loc[balanced_data['label'] == label] # gets only values with this specific label \n",
    "    text = parse(sub_df['text'].tolist(), stopwords)\n",
    "    wc = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(text)\n",
    "    # plot the WordCloud image  \n",
    "    plt.subplot(2, 3, label+1)     \n",
    "    plt.title(EMOTION_KEY[label])                \n",
    "    # plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/balanced_data.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "valid = pd.read_csv('data/validation.csv')\n",
    "\n",
    "EMOTION_KEY = {0: 'anger', 1: 'fear', 2: 'joy', 3:'love', 4:'sadness', 5: 'surprise'}\n",
    "train['label_text'] = train['label'].map(EMOTION_KEY)\n",
    "test['label_text'] = test['label'].map(EMOTION_KEY)\n",
    "valid['label_text'] = valid['label'].map(EMOTION_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text into sequences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['text'])\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the text sequences\n",
    "max_length = max([len(seq) for seq in train_sequences])\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a simple feed-forward neural network using sequential dense layers\n",
    "def simple_ffn(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=input_shape),\n",
    "        Dense(output_shape, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# a recurrent neural network model\n",
    "def rnn_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=64, input_length=input_shape),\n",
    "        LSTM(64),\n",
    "        Dense(output_shape, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# a convolutional neural network model\n",
    "def cnn_model(input_shape, output_shape):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=64, input_length=input_shape),\n",
    "        Conv1D(64, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Flatten(),\n",
    "        Dense(output_shape, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to train the model and output evaluation of training based on the history\n",
    "def train_and_evaluate(model, train_data, train_labels, valid_data, valid_labels, test_data, test_labels):\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(train_data, train_labels, epochs=10, validation_data=(valid_data, valid_labels), verbose=2)\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels, verbose=2)\n",
    "    return history, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_padded, train['label'].values\n",
    "X_test, y_test = test_padded, test['label'].values\n",
    "X_valid, y_valid = valid_padded, valid['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "output_shape = len(train['label'].unique())\n",
    "ffn_model = simple_ffn(input_shape, output_shape)\n",
    "ffn_history, ffn_loss, ffn_accuracy = train_and_evaluate(ffn_model, X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_input_shape = X_train.shape[1]\n",
    "rnn_model = rnn_model(rnn_input_shape, output_shape)\n",
    "rnn_history, rnn_loss, rnn_accuracy = train_and_evaluate(rnn_model, X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = cnn_model(rnn_input_shape, output_shape)\n",
    "cnn_history, cnn_loss, cnn_accuracy = train_and_evaluate(cnn_model, X_train, y_train, X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple Feedforward Neural Network - Test Loss:\", ffn_loss, \"Test Accuracy:\", ffn_accuracy)\n",
    "print(\"RNN Model - Test Loss:\", rnn_loss, \"Test Accuracy:\", rnn_accuracy)\n",
    "print(\"CNN Model - Test Loss:\", cnn_loss, \"Test Accuracy:\", cnn_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model_name):\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(model_name)\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(ffn_history, 'Simple Feed-Forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(rnn_history, 'Recurrent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(cnn_history, 'Convolutional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_model.save('saved_models/ffn_model.h5')\n",
    "rnn_model.save('saved_models/rnn_model.h5')\n",
    "cnn_model.save('saved_models/cnn_model.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "with open('saved_models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Bi-gram Model\n",
    "We want to explore how the different neural networks and different emotions compare ot a simple vanilla bigram model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Util funcs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "  \"\"\"Creates n-grams for the given token sequence.\n",
    "  Args:\n",
    "    tokens (list): a list of tokens as strings\n",
    "    n (int): the length of n-grams to create\n",
    "\n",
    "  Returns:\n",
    "    list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "  \"\"\"\n",
    "  lst = []\n",
    "  for i in range(len(tokens)): \n",
    "    ngrams = []\n",
    "    for j in range(i, i+n): \n",
    "      try: \n",
    "        ngrams.append(tokens[j])\n",
    "      except: \n",
    "        break\n",
    "    if len(ngrams) == n: \n",
    "      lst.append(tuple(ngrams))\n",
    "  return lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line: str, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize a single string. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    line (str): text to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - a single line tokenized\n",
    "  \"\"\"\n",
    "  inner_pieces = None\n",
    "  if by_char:\n",
    "    inner_pieces = list(line)\n",
    "  else:\n",
    "    # otherwise split on white space\n",
    "    inner_pieces = line.split()\n",
    "\n",
    "  if ngram == 1:\n",
    "    tokens = [sentence_begin] + inner_pieces + [sentence_end]\n",
    "  else:\n",
    "    tokens = ([sentence_begin] * (ngram - 1)) + inner_pieces + ([sentence_end] * (ngram - 1))\n",
    "  # always count the unigrams\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data: list, ngram: int, \n",
    "                   by_char: bool = True, \n",
    "                   sentence_begin: str=SENTENCE_BEGIN, \n",
    "                   sentence_end: str=SENTENCE_END):\n",
    "  \"\"\"\n",
    "  Tokenize each line in a list of strings. Glue on the appropriate number of \n",
    "  sentence begin tokens and sentence end tokens (ngram - 1), except\n",
    "  for the case when ngram == 1, when there will be one sentence begin\n",
    "  and one sentence end token.\n",
    "  Args:\n",
    "    data (list): list of strings to tokenize\n",
    "    ngram (int): ngram preparation number\n",
    "    by_char (bool): default value True, if True, tokenize by character, if\n",
    "      False, tokenize by whitespace\n",
    "    sentence_begin (str): sentence begin token value\n",
    "    sentence_end (str): sentence end token value\n",
    "\n",
    "  Returns:\n",
    "    list of strings - all lines tokenized as one large list\n",
    "  \"\"\"\n",
    "  total = []\n",
    "  # also glue on sentence begin and end items\n",
    "  for line in data:\n",
    "    line = line.strip()\n",
    "    # skip empty lines\n",
    "    if len(line) == 0:\n",
    "      continue\n",
    "    tokens = tokenize_line(line, ngram, by_char, sentence_begin, sentence_end)\n",
    "    total += tokens\n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('data/balanced_training.txt', 'r') as file: \n",
    "    for line in file: \n",
    "        text.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and generating: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "df = pd.read_csv('data/balanced_data.csv')\n",
    "\n",
    "emotion_dfs = {}\n",
    "for label in EMOTION_KEY: \n",
    "    curr = df.loc[df['label'] == label]\n",
    "    emotion_dfs[label] = curr\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 2\n",
    "gen_sents = {}\n",
    "for label, df_ in tqdm(emotion_dfs.items()): \n",
    "    # tokenize by word\n",
    "    toks = tokenize(list(df_['text']), ngram, by_char=False)\n",
    "    lm = LanguageModel(ngram)\n",
    "    lm.train(toks)\n",
    "    gen_sents[label] = lm.generate(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vanilla_emotion_results.txt', 'w') as file: \n",
    "    json.dump(gen_sents, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 2\n",
    "gen_sents_char = {}\n",
    "for label, df_ in tqdm(emotion_dfs.items()): \n",
    "    # tokenize by word\n",
    "    toks = tokenize(list(df_['text']), ngram, by_char=True)\n",
    "    lm_char = LanguageModel(ngram)\n",
    "    lm_char.train(toks)\n",
    "    gen_sents_char[label] = lm_char.generate(100)\n",
    "\n",
    "with open('vanilla_emotion_results_char.txt', 'w') as file: \n",
    "    json.dump(gen_sents_char, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_char_gen = {}\n",
    "\n",
    "for key, value in gen_sents_char.items(): \n",
    "    sents = []\n",
    "    for sent in value: \n",
    "        sent = \"\".join(sent).replace('<s>', '').replace('</s>', '')\n",
    "        sents.append(sent)\n",
    "    new_char_gen[key] = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('char_vanilla_emotion_res.txt', 'w') as file:             \n",
    "    json.dump(new_char_gen, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
